{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "from modeling.efficient_3dsam.efficient_3dsam_encoder import ImageEncoderViT_3d\n",
    "from functools import partial\n",
    "import torch\n",
    "from modeling.efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_encoder ImageEncoderViT_3d(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (slice_embed): Conv3d(192, 192, kernel_size=(1, 1, 16), stride=(1, 1, 16), groups=192)\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block_3d(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention_3d(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (lin1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (lin2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "      )\n",
      "      (adapter): Adapter(\n",
      "        (linear1): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96)\n",
      "        (linear2): Linear(in_features=96, out_features=192, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (neck_3d): ModuleList(\n",
      "    (0-3): 4 x Sequential(\n",
      "      (0): Conv3d(768, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (1): LayerNorm3d()\n",
      "      (2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (3): LayerNorm3d()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['pos_embed_3d', 'depth_embed', 'slice_embed.weight', 'slice_embed.bias', 'blocks.0.attn_mask', 'blocks.0.attn.rel_pos_h', 'blocks.0.attn.rel_pos_w', 'blocks.0.attn.rel_pos_d', 'blocks.0.attn.lr', 'blocks.0.mlp.lin1.weight', 'blocks.0.mlp.lin1.bias', 'blocks.0.mlp.lin2.weight', 'blocks.0.mlp.lin2.bias', 'blocks.0.adapter.linear1.weight', 'blocks.0.adapter.linear1.bias', 'blocks.0.adapter.conv.weight', 'blocks.0.adapter.conv.bias', 'blocks.0.adapter.linear2.weight', 'blocks.0.adapter.linear2.bias', 'blocks.1.attn.rel_pos_h', 'blocks.1.attn.rel_pos_w', 'blocks.1.attn.rel_pos_d', 'blocks.1.attn.lr', 'blocks.1.mlp.lin1.weight', 'blocks.1.mlp.lin1.bias', 'blocks.1.mlp.lin2.weight', 'blocks.1.mlp.lin2.bias', 'blocks.1.adapter.linear1.weight', 'blocks.1.adapter.linear1.bias', 'blocks.1.adapter.conv.weight', 'blocks.1.adapter.conv.bias', 'blocks.1.adapter.linear2.weight', 'blocks.1.adapter.linear2.bias', 'blocks.2.attn_mask', 'blocks.2.attn.rel_pos_h', 'blocks.2.attn.rel_pos_w', 'blocks.2.attn.rel_pos_d', 'blocks.2.attn.lr', 'blocks.2.mlp.lin1.weight', 'blocks.2.mlp.lin1.bias', 'blocks.2.mlp.lin2.weight', 'blocks.2.mlp.lin2.bias', 'blocks.2.adapter.linear1.weight', 'blocks.2.adapter.linear1.bias', 'blocks.2.adapter.conv.weight', 'blocks.2.adapter.conv.bias', 'blocks.2.adapter.linear2.weight', 'blocks.2.adapter.linear2.bias', 'blocks.3.attn.rel_pos_h', 'blocks.3.attn.rel_pos_w', 'blocks.3.attn.rel_pos_d', 'blocks.3.attn.lr', 'blocks.3.mlp.lin1.weight', 'blocks.3.mlp.lin1.bias', 'blocks.3.mlp.lin2.weight', 'blocks.3.mlp.lin2.bias', 'blocks.3.adapter.linear1.weight', 'blocks.3.adapter.linear1.bias', 'blocks.3.adapter.conv.weight', 'blocks.3.adapter.conv.bias', 'blocks.3.adapter.linear2.weight', 'blocks.3.adapter.linear2.bias', 'blocks.4.attn_mask', 'blocks.4.attn.rel_pos_h', 'blocks.4.attn.rel_pos_w', 'blocks.4.attn.rel_pos_d', 'blocks.4.attn.lr', 'blocks.4.mlp.lin1.weight', 'blocks.4.mlp.lin1.bias', 'blocks.4.mlp.lin2.weight', 'blocks.4.mlp.lin2.bias', 'blocks.4.adapter.linear1.weight', 'blocks.4.adapter.linear1.bias', 'blocks.4.adapter.conv.weight', 'blocks.4.adapter.conv.bias', 'blocks.4.adapter.linear2.weight', 'blocks.4.adapter.linear2.bias', 'blocks.5.attn.rel_pos_h', 'blocks.5.attn.rel_pos_w', 'blocks.5.attn.rel_pos_d', 'blocks.5.attn.lr', 'blocks.5.mlp.lin1.weight', 'blocks.5.mlp.lin1.bias', 'blocks.5.mlp.lin2.weight', 'blocks.5.mlp.lin2.bias', 'blocks.5.adapter.linear1.weight', 'blocks.5.adapter.linear1.bias', 'blocks.5.adapter.conv.weight', 'blocks.5.adapter.conv.bias', 'blocks.5.adapter.linear2.weight', 'blocks.5.adapter.linear2.bias', 'blocks.6.attn_mask', 'blocks.6.attn.rel_pos_h', 'blocks.6.attn.rel_pos_w', 'blocks.6.attn.rel_pos_d', 'blocks.6.attn.lr', 'blocks.6.mlp.lin1.weight', 'blocks.6.mlp.lin1.bias', 'blocks.6.mlp.lin2.weight', 'blocks.6.mlp.lin2.bias', 'blocks.6.adapter.linear1.weight', 'blocks.6.adapter.linear1.bias', 'blocks.6.adapter.conv.weight', 'blocks.6.adapter.conv.bias', 'blocks.6.adapter.linear2.weight', 'blocks.6.adapter.linear2.bias', 'blocks.7.attn.rel_pos_h', 'blocks.7.attn.rel_pos_w', 'blocks.7.attn.rel_pos_d', 'blocks.7.attn.lr', 'blocks.7.mlp.lin1.weight', 'blocks.7.mlp.lin1.bias', 'blocks.7.mlp.lin2.weight', 'blocks.7.mlp.lin2.bias', 'blocks.7.adapter.linear1.weight', 'blocks.7.adapter.linear1.bias', 'blocks.7.adapter.conv.weight', 'blocks.7.adapter.conv.bias', 'blocks.7.adapter.linear2.weight', 'blocks.7.adapter.linear2.bias', 'blocks.8.attn_mask', 'blocks.8.attn.rel_pos_h', 'blocks.8.attn.rel_pos_w', 'blocks.8.attn.rel_pos_d', 'blocks.8.attn.lr', 'blocks.8.mlp.lin1.weight', 'blocks.8.mlp.lin1.bias', 'blocks.8.mlp.lin2.weight', 'blocks.8.mlp.lin2.bias', 'blocks.8.adapter.linear1.weight', 'blocks.8.adapter.linear1.bias', 'blocks.8.adapter.conv.weight', 'blocks.8.adapter.conv.bias', 'blocks.8.adapter.linear2.weight', 'blocks.8.adapter.linear2.bias', 'blocks.9.attn.rel_pos_h', 'blocks.9.attn.rel_pos_w', 'blocks.9.attn.rel_pos_d', 'blocks.9.attn.lr', 'blocks.9.mlp.lin1.weight', 'blocks.9.mlp.lin1.bias', 'blocks.9.mlp.lin2.weight', 'blocks.9.mlp.lin2.bias', 'blocks.9.adapter.linear1.weight', 'blocks.9.adapter.linear1.bias', 'blocks.9.adapter.conv.weight', 'blocks.9.adapter.conv.bias', 'blocks.9.adapter.linear2.weight', 'blocks.9.adapter.linear2.bias', 'blocks.10.attn_mask', 'blocks.10.attn.rel_pos_h', 'blocks.10.attn.rel_pos_w', 'blocks.10.attn.rel_pos_d', 'blocks.10.attn.lr', 'blocks.10.mlp.lin1.weight', 'blocks.10.mlp.lin1.bias', 'blocks.10.mlp.lin2.weight', 'blocks.10.mlp.lin2.bias', 'blocks.10.adapter.linear1.weight', 'blocks.10.adapter.linear1.bias', 'blocks.10.adapter.conv.weight', 'blocks.10.adapter.conv.bias', 'blocks.10.adapter.linear2.weight', 'blocks.10.adapter.linear2.bias', 'blocks.11.attn.rel_pos_h', 'blocks.11.attn.rel_pos_w', 'blocks.11.attn.rel_pos_d', 'blocks.11.attn.lr', 'blocks.11.mlp.lin1.weight', 'blocks.11.mlp.lin1.bias', 'blocks.11.mlp.lin2.weight', 'blocks.11.mlp.lin2.bias', 'blocks.11.adapter.linear1.weight', 'blocks.11.adapter.linear1.bias', 'blocks.11.adapter.conv.weight', 'blocks.11.adapter.conv.bias', 'blocks.11.adapter.linear2.weight', 'blocks.11.adapter.linear2.bias', 'neck_3d.0.0.weight', 'neck_3d.0.1.weight', 'neck_3d.0.1.bias', 'neck_3d.0.2.weight', 'neck_3d.0.3.weight', 'neck_3d.0.3.bias', 'neck_3d.1.0.weight', 'neck_3d.1.1.weight', 'neck_3d.1.1.bias', 'neck_3d.1.2.weight', 'neck_3d.1.3.weight', 'neck_3d.1.3.bias', 'neck_3d.2.0.weight', 'neck_3d.2.1.weight', 'neck_3d.2.1.bias', 'neck_3d.2.2.weight', 'neck_3d.2.3.weight', 'neck_3d.2.3.bias', 'neck_3d.3.0.weight', 'neck_3d.3.1.weight', 'neck_3d.3.1.bias', 'neck_3d.3.2.weight', 'neck_3d.3.3.weight', 'neck_3d.3.3.bias'], unexpected_keys=['pos_embed', 'neck.0.weight', 'neck.1.weight', 'neck.1.bias', 'neck.2.weight', 'neck.3.weight', 'neck.3.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficient_sam = build_efficient_sam_vitt()\n",
    "\n",
    "img_encoder = ImageEncoderViT_3d(\n",
    "    depth=12,\n",
    "    embed_dim=192,\n",
    "    img_size=1024,\n",
    "    mlp_ratio=4,\n",
    "    norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "    num_heads=12,\n",
    "    patch_size=16,\n",
    "    qkv_bias=True,\n",
    "    use_rel_pos=True,\n",
    "    global_attn_indexes=[2, 5, 8, 11],\n",
    "    window_size=14,\n",
    "    cubic_window_size=8,\n",
    "    out_chans=256,\n",
    "    num_slice = 16)\n",
    "\n",
    "print(\"img_encoder\", img_encoder)\n",
    "img_encoder.load_state_dict(efficient_sam.image_encoder.state_dict(), strict=False)\n",
    "del efficient_sam\n",
    "# img_encoder.to(device)\n",
    "\n",
    "for p in img_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "img_encoder.depth_embed.requires_grad = True\n",
    "for p in img_encoder.slice_embed.parameters():\n",
    "    p.requires_grad = True\n",
    "for i in img_encoder.blocks:\n",
    "    for p in i.norm1.parameters():\n",
    "        p.requires_grad = True\n",
    "    for p in i.adapter.parameters():\n",
    "        p.requires_grad = True\n",
    "    for p in i.norm2.parameters():\n",
    "        p.requires_grad = True\n",
    "    i.attn.rel_pos_d = nn.parameter.Parameter(0.5 * (i.attn.rel_pos_h + i.attn.rel_pos_w), requires_grad=True)\n",
    "for i in img_encoder.neck_3d:\n",
    "    for p in i.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "prompt_encoder_list = []\n",
    "parameter_list = []\n",
    "for i in range(4):\n",
    "    prompt_encoder = PromptEncoder(transformer=TwoWayTransformer(depth=2,\n",
    "                                                                embedding_dim=256,\n",
    "                                                                mlp_dim=2048,\n",
    "                                                                num_heads=8))\n",
    "    prompt_encoder.to(device)\n",
    "    prompt_encoder_list.append(prompt_encoder)\n",
    "    parameter_list.extend([i for i in prompt_encoder.parameters() if i.requires_grad == True])\n",
    "\n",
    "mask_decoder = VIT_MLAHead(img_size=96, num_classes=2)\n",
    "mask_decoder.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = sam_model_registry[\"vit_b\"](checkpoint=\"weights/sam_vit_b_01ec64.pth\")\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "img_encoder = ImageEncoderViT_3d(\n",
    "    depth=12,\n",
    "    embed_dim=768,\n",
    "    img_size=1024,\n",
    "    mlp_ratio=4,\n",
    "    norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "    num_heads=12,\n",
    "    patch_size=16,\n",
    "    qkv_bias=True,\n",
    "    use_rel_pos=True,\n",
    "    global_attn_indexes=[2, 5, 8, 11],\n",
    "    window_size=14,\n",
    "    cubic_window_size=8,\n",
    "    out_chans=256,\n",
    "    num_slice = 16)\n",
    "\n",
    "img_encoder.load_state_dict(mask_generator.predictor.model.image_encoder.state_dict(), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
